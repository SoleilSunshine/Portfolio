{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Natural Language Processing - Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Natural Language Processing*** assists us in identifying relationships and patterns in our writing, and can make our predictions very interesting. Below I conduct a sentiment analysis, to determine which tweets have a positive connotation or a negative connotation. For a better understanding of sentiment analysis, take a look at this paper written by Stanford professors and researchers. The live demo link shows an excellent representation of sentiment analysis (http://nlp.stanford.edu/sentiment/index.html).\n",
    "\n",
    "\n",
    "\n",
    "***Problem:*** Conduct a Sentiment Analysis on the following tweets to determine their polarity, how often each unique word is mentioned in all tweets in this dataset, and the unique words tf-idf (term frequency - inverse document frequency).\n",
    "<br><br>\n",
    "***Data:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1467933112</th>\n",
       "      <td>0</td>\n",
       "      <td>the angel is going to miss the athlete this we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2323395086</th>\n",
       "      <td>0</td>\n",
       "      <td>It looks as though Shaq is getting traded to C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467968979</th>\n",
       "      <td>0</td>\n",
       "      <td>@clarianne APRIL 9TH ISN'T COMING SOON ENOUGH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990283756</th>\n",
       "      <td>0</td>\n",
       "      <td>drinking a McDonalds coffee and not understand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988884918</th>\n",
       "      <td>0</td>\n",
       "      <td>So dissapointed Taylor Swift doesnt have a Twi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            polarity                                              tweet\n",
       "id                                                                     \n",
       "1467933112         0  the angel is going to miss the athlete this we...\n",
       "2323395086         0  It looks as though Shaq is getting traded to C...\n",
       "1467968979         0     @clarianne APRIL 9TH ISN'T COMING SOON ENOUGH \n",
       "1990283756         0  drinking a McDonalds coffee and not understand...\n",
       "1988884918         0  So dissapointed Taylor Swift doesnt have a Twi..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv('tweet_data.csv', sep=';', index_col=0)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This single tweet below is identified as a negative statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polarity                                                    0\n",
       "tweet       So dissapointed Taylor Swift doesnt have a Twi...\n",
       "Name: 1988884918, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.loc[1988884918]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the angel is going to miss the athlete this weekend ',\n",
       "       \"It looks as though Shaq is getting traded to Cleveland to play w/ LeBron... Too bad for Suns' fans. The Big Cactus is no more \",\n",
       "       \"@clarianne APRIL 9TH ISN'T COMING SOON ENOUGH \", ...,\n",
       "       '@jordanhowell lol only a PSP, had a game boy but my cousin lost it  &amp; theres a N64 around',\n",
       "       'Good morning everyone!  It is such a beautiful day here in New England!  ',\n",
       "       'hey guess was @magicmanil the Lakers won and KOBE is mvp  just thought I would tell ya haha'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets = tweets_df['tweet'].values\n",
    "train_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>***CountVectorizer:*** identify unique words in tweet and count them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initializes vectorizer: English stop words & convert to lower case\n",
    "vectorizer = CountVectorizer(stop_words='english', lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating list of unique words\n",
    "vectorizer.fit(train_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the angel is going to miss the athlete this weekend \n",
      "  (0, 319)\t1\n",
      "  (0, 414)\t1\n",
      "  (0, 1980)\t1\n",
      "  (0, 3062)\t1\n",
      "  (0, 5002)\t1\n"
     ]
    }
   ],
   "source": [
    "#Creates new sparse matrix. For each unique word, counts how many are in the list.\n",
    "X_train = vectorizer.transform(train_tweets)\n",
    "print(train_tweets[0])\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414\n"
     ]
    }
   ],
   "source": [
    "#Identify coded number of word 'athlete'\n",
    "print(vectorizer.vocabulary_.get('athlete'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At starbucks with my new sister  learning her new phone.\n",
      "  (0, 2685)\t1\n",
      "  (0, 3218)\t2\n",
      "  (0, 3472)\t1\n",
      "  (0, 4138)\t1\n",
      "  (0, 4313)\t1\n",
      "3218\n"
     ]
    }
   ],
   "source": [
    "print(train_tweets[186])\n",
    "print(X_train[186])\n",
    "print(vectorizer.vocabulary_.get('new'))\n",
    "#Here we can see that the word new is verbalized twice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>***TfidfTransformer:*** identifies learning score of each unique word. The learning score may be high because the word is less frequent in other tweets, and more unique to this tweet. If it is low, the unique word is more frequent in other tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = TfidfTransformer()\n",
    "transformer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_tfidf = transformer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At starbucks with my new sister  learning her new phone.\n",
      "  (0, 4313)\t0.285913376715\n",
      "  (0, 4138)\t0.422150263806\n",
      "  (0, 3472)\t0.386021064679\n",
      "  (0, 3218)\t0.573798111469\n",
      "  (0, 2685)\t0.511650428206\n",
      "3218\n",
      "4313\n",
      "2685\n"
     ]
    }
   ],
   "source": [
    "print(train_tweets[186])\n",
    "print(X_train_tfidf[186])\n",
    "print(vectorizer.vocabulary_.get('new'))\n",
    "print(vectorizer.vocabulary_.get('starbucks'))\n",
    "print(vectorizer.vocabulary_.get('learning'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starbucks has been mentioned 70 times in the tweets in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(['starbucks' in tweet.lower() for tweet in train_tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We have ['starbucks', 'new', sister', 'learning', 'new', 'phone']\n",
    "#tf(starbucks) = 1/6\n",
    "#tf(new) = 2/6\n",
    "#tfidf = tf(1/df +1)\n",
    "#tfidf = tf/df - tf will be almost the same for each unique word. Therefore the df\n",
    "#must be high. A lower score must mean that the unique word is common in other documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "87\n",
      "9\n",
      "1\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "print(sum(['starbucks' in tweet.lower() for tweet in train_tweets]))\n",
    "print(sum(['new' in tweet.lower() for tweet in train_tweets]))\n",
    "print(sum(['sister' in tweet.lower() for tweet in train_tweets]))\n",
    "print(sum(['learning' in tweet.lower() for tweet in train_tweets]))\n",
    "print(sum(['phone' in tweet.lower() for tweet in train_tweets]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for x in train_tweets:\n",
    "#    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the polarity values. They seem to be 0 and 4. Make them 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 4 4]\n"
     ]
    }
   ],
   "source": [
    "y = tweets_df['polarity'].values\n",
    "print(y)\n",
    "#[tweet for tweet in train_tweets if '!!!' in tweet.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1680347120</th>\n",
       "      <td>1</td>\n",
       "      <td>@ mcdonalds with my litto sis aka cuzin lol cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835259469</th>\n",
       "      <td>1</td>\n",
       "      <td>@AnnaSaccone Love your new cards!   I would de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983068285</th>\n",
       "      <td>1</td>\n",
       "      <td>@supricky06 that was one of the most enjoyable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559842363</th>\n",
       "      <td>1</td>\n",
       "      <td>Dallas vegas goodness  http://twitpic.com/3lzt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999078293</th>\n",
       "      <td>1</td>\n",
       "      <td>@JBsFanArgentina Hey I luv this pic!!! was ama...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            polarity                                              tweet\n",
       "id                                                                     \n",
       "1680347120         1  @ mcdonalds with my litto sis aka cuzin lol cr...\n",
       "1835259469         1  @AnnaSaccone Love your new cards!   I would de...\n",
       "1983068285         1  @supricky06 that was one of the most enjoyable...\n",
       "1559842363         1  Dallas vegas goodness  http://twitpic.com/3lzt...\n",
       "1999078293         1  @JBsFanArgentina Hey I luv this pic!!! was ama..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[tweets_df['polarity']>0]\n",
    "tweets_df['polarity'] = tweets_df['polarity'].replace(4, 1)\n",
    "tweets_df[tweets_df['polarity']>0].head()\n",
    "\n",
    "#We just replaced the 4's with 1's. These are the positive tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['polarity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 5220)\n",
      "(2034,)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "X = X_train_tfidf.toarray()    #converts sparce matrix to ndarray\n",
    "y = tweets_df['polarity'].values\n",
    "print(X.shape), print(y.shape)\n",
    "print(X)\n",
    "#X 2034 tweets and 5220 unique words\n",
    "#y is one sentiment value for every tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>***Logistic Regression***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.82474227,  0.82743363,  0.80945347])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=1000)   #Large C means no regularization, small c means\n",
    "#all magnitudes of weights to the cost\n",
    "logreg.fit(X,y)    #computes coefficients depending on our train features\n",
    "\n",
    "cross_val_score(logreg, X, y, cv=3)   #cv = cross validation K folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>***Multinomial Naive Bayes***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.73490427,  0.73303835,  0.73855244])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()     #Naive Bayes\n",
    "nb.fit(X, y)\n",
    "\n",
    "cross_val_score(nb, X, y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-20.662861614009636, 18.036108603583237)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#small c gives smaller coefficients\n",
    "#If C = 10000, it will go from -28 to 24. Regularizing tries to minimize your coefficients. Finding max and min helps.\n",
    "logreg.coef_.min(), logreg.coef_.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>***Improvements to make:*** to improve our score, we can try other models (neural networks), try stemming, or removing stop words.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hanging out with biology til 4am woo  !\n",
      "  (0, 5087)\t0.442275696252\n",
      "  (0, 4637)\t0.417479175819\n",
      "  (0, 2117)\t0.428629186309\n",
      "  (0, 585)\t0.459869107486\n",
      "  (0, 107)\t0.484665627919\n",
      "5087\n"
     ]
    }
   ],
   "source": [
    "print(train_tweets[100])\n",
    "print(X_train_tfidf[100])\n",
    "print(vectorizer.vocabulary_.get('woo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.9179434329172311"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.coef_[0][5087]\n",
    "#strong positive coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hmm im usually dead rite now...ugh skool monday..no more oprah ellen or kathie lee and hoda     i wish u went to skool for a millisecond!!\n",
      "  (0, 5061)\t0.157668011698\n",
      "  (0, 5015)\t0.19748714517\n",
      "  (0, 4879)\t0.236782492398\n",
      "  (0, 4809)\t0.189948566976\n",
      "  (0, 4161)\t0.508079976472\n",
      "  (0, 3827)\t0.267738033303\n",
      "  (0, 3359)\t0.15172580304\n",
      "  (0, 3099)\t0.216924980458\n",
      "  (0, 3042)\t0.267738033303\n",
      "  (0, 2696)\t0.244321070592\n",
      "  (0, 2548)\t0.254039988236\n",
      "  (0, 2342)\t0.149105611639\n",
      "  (0, 2230)\t0.254039988236\n",
      "  (0, 2227)\t0.254039988236\n",
      "  (0, 1554)\t0.236782492398\n",
      "  (0, 1298)\t0.201998307776\n",
      "4809\n"
     ]
    }
   ],
   "source": [
    "print(train_tweets[36])\n",
    "print(X_train_tfidf[36])\n",
    "print(vectorizer.vocabulary_.get('ugh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.4697648471867515"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.coef_[0][4809]\n",
    "#strong negative coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', ngram_range = (1,2), #Unigrams+Bigrams\n",
    "                             lowercase=True)\n",
    "vectorizer.fit(train_tweets)\n",
    "X_train = vectorizer.transform(train_tweets)\n",
    "#If we did (2,2) that would just be bigrams\n",
    "#So what we have as 1,2 is 'finals' + 'baby' + 'yeah' + 'finals baby' + 'baby yeah' = 5 instead\n",
    "#of 3 from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At starbucks with my new sister  learning her new phone.\n",
      "  (0, 8787)\t1\n",
      "  (0, 8788)\t1\n",
      "  (0, 10896)\t2\n",
      "  (0, 10927)\t1\n",
      "  (0, 10932)\t1\n",
      "  (0, 11854)\t1\n",
      "  (0, 13933)\t1\n",
      "  (0, 13937)\t1\n",
      "  (0, 14435)\t1\n",
      "  (0, 14470)\t1\n",
      "10896\n",
      "14435\n",
      "8787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2034, 18051)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_tweets[186])\n",
    "print(X_train[186])\n",
    "print(vectorizer.vocabulary_.get('new'))\n",
    "print(vectorizer.vocabulary_.get('starbucks'))\n",
    "print(vectorizer.vocabulary_.get('learning'))\n",
    "X_train.shape   #Now we have 18000 paired words(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "transformer.fit(X_train)\n",
    "X_train_tfidf = transformer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 18051)\n",
      "(2034,)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "X = X_train_tfidf.toarray()    #converts sparce matrix to ndarray\n",
    "y = tweets_df['polarity'].values\n",
    "print(X.shape), print(y.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>***Logistic Regression***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.80265096,  0.81563422,  0.79468242])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=1000)\n",
    "logreg.fit(X,y)    #computes coefficients depending our train features\n",
    "\n",
    "cross_val_score(logreg, X, y, cv=3)   #cv = cross validation K folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[[  9.99939712e-01   6.02883632e-05]]\n",
      "  (0, 3374)\t1\n",
      "3374\n",
      "-7.23869236133\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"I'm crying because it's gloomy out.\"]\n",
    "sentence_counts = vectorizer.transform(sentence)\n",
    "sentence_tfidf = transformer.transform(sentence_counts)\n",
    "print(logreg.predict(sentence_tfidf.toarray()))  # since it is 0, means negative\n",
    "print(logreg.predict_proba(sentence_tfidf.toarray()))\n",
    "\n",
    "print(sentence_counts)   #shows only one word was in original vocabulary (probably crying)\n",
    "print(vectorizer.vocabulary_.get('crying'))\n",
    "print(logreg.coef_[0][3374])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[[ 0.80005783  0.19994217]]\n",
      "  (0, 11230)\t1\n",
      "  (0, 15991)\t1\n",
      "  (0, 17905)\t1\n",
      "15991\n",
      "17905\n",
      "11230\n",
      "-0.986603939374\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"Yes! Obama is in town!\"]\n",
    "sentence_counts = vectorizer.transform(sentence)\n",
    "sentence_tfidf = transformer.transform(sentence_counts)\n",
    "print(logreg.predict(sentence_tfidf.toarray()))  # since it is 0, means negative (predicts sarcasm? or error?)\n",
    "print(logreg.predict_proba(sentence_tfidf.toarray()))\n",
    "\n",
    "print(sentence_counts)   #shows three words in original vocabulary (Obama, town, Yes).\n",
    "print(vectorizer.vocabulary_.get('town'))\n",
    "print(vectorizer.vocabulary_.get('yes'))\n",
    "print(vectorizer.vocabulary_.get('obama'))\n",
    "print(logreg.coef_[0][15991])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
